{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 学習と検証の実施\n",
    "\n",
    "- 本ファイルでは、PSPNetの学習と検証の実施を行います。AWSのGPUマシンで計算します。\n",
    "- p2.xlargeで●時間かかります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習目標\n",
    "\n",
    "1.\tPSPNetの学習と検証を実装できるようになる\n",
    "2.\tセマンティックセグメンテーションのファインチューニングを理解する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "\n",
    "- 本書に従い学習済みモデルのファイル「pspnet50_ADE20K.pth」をダウンロードし、フォルダ「weights」に用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# ファイルパスリスト作成\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "# (RGB)の色の平均値と標準偏差\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークモデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了：学習済みの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ファインチューニングでPSPNetを作成\n",
    "# ADE20Kデータセットの学習済みモデルを使用、ADE20Kはクラス数が150です\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "# ADE20K学習済みパラメータをロード\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 分類用の畳み込み層を、出力数21のものにつけかえる\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(\n",
    "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "net.aux.classification = nn.Conv2d(\n",
    "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する。\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)\n",
    "\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "class PSPLoss(nn.Module):\n",
    "    \"\"\"PSPNetの損失関数のクラスです。\"\"\"\n",
    "\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight  # aux_lossの重み\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : PSPNetの出力(tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
    "\n",
    "        targets : [num_batch, 475, 4755]\n",
    "            正解のアノテーション情報\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : テンソル\n",
    "            損失の値\n",
    "        \"\"\"\n",
    "\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化手法を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングなので、学習率は小さく\n",
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "# スケジューラーの設定\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・検証を実施する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # イタレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                scheduler.step()  # 最適化schedulerの更新\n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イタレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    # 最後のネットワークを保存する\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------\n",
      "Epoch 1/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 10 || Loss: 0.3809 || 10iter: 85.7882 sec.\n",
      "イタレーション 20 || Loss: 0.2182 || 10iter: 54.1508 sec.\n",
      "イタレーション 30 || Loss: 0.1513 || 10iter: 53.7752 sec.\n",
      "イタレーション 40 || Loss: 0.1653 || 10iter: 54.4371 sec.\n",
      "イタレーション 50 || Loss: 0.0889 || 10iter: 53.9188 sec.\n",
      "イタレーション 60 || Loss: 0.0728 || 10iter: 54.0658 sec.\n",
      "イタレーション 70 || Loss: 0.1163 || 10iter: 53.6033 sec.\n",
      "イタレーション 80 || Loss: 0.1338 || 10iter: 53.7854 sec.\n",
      "イタレーション 90 || Loss: 0.2167 || 10iter: 54.4044 sec.\n",
      "イタレーション 100 || Loss: 0.0897 || 10iter: 54.1622 sec.\n",
      "イタレーション 110 || Loss: 0.1403 || 10iter: 53.9190 sec.\n",
      "イタレーション 120 || Loss: 0.0668 || 10iter: 54.2026 sec.\n",
      "イタレーション 130 || Loss: 0.1241 || 10iter: 53.9137 sec.\n",
      "イタレーション 140 || Loss: 0.1469 || 10iter: 53.6589 sec.\n",
      "イタレーション 150 || Loss: 0.0788 || 10iter: 54.2437 sec.\n",
      "イタレーション 160 || Loss: 0.1428 || 10iter: 54.2170 sec.\n",
      "イタレーション 170 || Loss: 0.2082 || 10iter: 54.1832 sec.\n",
      "イタレーション 180 || Loss: 0.1429 || 10iter: 54.1187 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.1762 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1124.2241 sec.\n",
      "-------------\n",
      "Epoch 2/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 190 || Loss: 0.0991 || 10iter: 36.4594 sec.\n",
      "イタレーション 200 || Loss: 0.0862 || 10iter: 54.1726 sec.\n",
      "イタレーション 210 || Loss: 0.0701 || 10iter: 54.0758 sec.\n",
      "イタレーション 220 || Loss: 0.0451 || 10iter: 54.1056 sec.\n",
      "イタレーション 230 || Loss: 0.0539 || 10iter: 54.0063 sec.\n",
      "イタレーション 240 || Loss: 0.1088 || 10iter: 54.0323 sec.\n",
      "イタレーション 250 || Loss: 0.0656 || 10iter: 54.0964 sec.\n",
      "イタレーション 260 || Loss: 0.0580 || 10iter: 53.9694 sec.\n",
      "イタレーション 270 || Loss: 0.0639 || 10iter: 53.8732 sec.\n",
      "イタレーション 280 || Loss: 0.0501 || 10iter: 53.9538 sec.\n",
      "イタレーション 290 || Loss: 0.0572 || 10iter: 54.1030 sec.\n",
      "イタレーション 300 || Loss: 0.1096 || 10iter: 54.1284 sec.\n",
      "イタレーション 310 || Loss: 0.0818 || 10iter: 54.0959 sec.\n",
      "イタレーション 320 || Loss: 0.0717 || 10iter: 53.9891 sec.\n",
      "イタレーション 330 || Loss: 0.0502 || 10iter: 54.0292 sec.\n",
      "イタレーション 340 || Loss: 0.0819 || 10iter: 53.4865 sec.\n",
      "イタレーション 350 || Loss: 0.0684 || 10iter: 54.0016 sec.\n",
      "イタレーション 360 || Loss: 0.0570 || 10iter: 54.0936 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.0913 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1091.7824 sec.\n",
      "-------------\n",
      "Epoch 3/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 370 || Loss: 0.0678 || 10iter: 18.3833 sec.\n",
      "イタレーション 380 || Loss: 0.0630 || 10iter: 54.2699 sec.\n",
      "イタレーション 390 || Loss: 0.0551 || 10iter: 53.7031 sec.\n",
      "イタレーション 400 || Loss: 0.0437 || 10iter: 54.2488 sec.\n",
      "イタレーション 410 || Loss: 0.0561 || 10iter: 54.1036 sec.\n",
      "イタレーション 420 || Loss: 0.0922 || 10iter: 54.0160 sec.\n",
      "イタレーション 430 || Loss: 0.1128 || 10iter: 54.0698 sec.\n",
      "イタレーション 440 || Loss: 0.0747 || 10iter: 54.0088 sec.\n",
      "イタレーション 450 || Loss: 0.0604 || 10iter: 54.0864 sec.\n",
      "イタレーション 460 || Loss: 0.0656 || 10iter: 54.1180 sec.\n",
      "イタレーション 470 || Loss: 0.0573 || 10iter: 53.9838 sec.\n",
      "イタレーション 480 || Loss: 0.0474 || 10iter: 54.0222 sec.\n",
      "イタレーション 490 || Loss: 0.0722 || 10iter: 54.1288 sec.\n",
      "イタレーション 500 || Loss: 0.0632 || 10iter: 53.9620 sec.\n",
      "イタレーション 510 || Loss: 0.0769 || 10iter: 54.0415 sec.\n",
      "イタレーション 520 || Loss: 0.0832 || 10iter: 54.1300 sec.\n",
      "イタレーション 530 || Loss: 0.0623 || 10iter: 54.0675 sec.\n",
      "イタレーション 540 || Loss: 0.0429 || 10iter: 54.0145 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:0.0779 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1092.7246 sec.\n",
      "-------------\n",
      "Epoch 4/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 550 || Loss: 0.1591 || 10iter: 0.3299 sec.\n",
      "イタレーション 560 || Loss: 0.0372 || 10iter: 54.1777 sec.\n",
      "イタレーション 570 || Loss: 0.0564 || 10iter: 54.0329 sec.\n",
      "イタレーション 580 || Loss: 0.0973 || 10iter: 54.4284 sec.\n",
      "イタレーション 590 || Loss: 0.0669 || 10iter: 53.9447 sec.\n",
      "イタレーション 600 || Loss: 0.0671 || 10iter: 54.0939 sec.\n",
      "イタレーション 610 || Loss: 0.0630 || 10iter: 54.2676 sec.\n",
      "イタレーション 620 || Loss: 0.0280 || 10iter: 54.0700 sec.\n",
      "イタレーション 630 || Loss: 0.1317 || 10iter: 54.0600 sec.\n",
      "イタレーション 640 || Loss: 0.0845 || 10iter: 54.0036 sec.\n",
      "イタレーション 650 || Loss: 0.0630 || 10iter: 54.0092 sec.\n",
      "イタレーション 660 || Loss: 0.0743 || 10iter: 54.1075 sec.\n",
      "イタレーション 670 || Loss: 0.0433 || 10iter: 54.1210 sec.\n",
      "イタレーション 680 || Loss: 0.1267 || 10iter: 54.1556 sec.\n",
      "イタレーション 690 || Loss: 0.0840 || 10iter: 54.0106 sec.\n",
      "イタレーション 700 || Loss: 0.0775 || 10iter: 54.0661 sec.\n",
      "イタレーション 710 || Loss: 0.0655 || 10iter: 54.0670 sec.\n",
      "イタレーション 720 || Loss: 0.0442 || 10iter: 54.0511 sec.\n",
      "イタレーション 730 || Loss: 0.0653 || 10iter: 54.1378 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:0.0708 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1093.1451 sec.\n",
      "-------------\n",
      "Epoch 5/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 740 || Loss: 0.0730 || 10iter: 42.0868 sec.\n",
      "イタレーション 750 || Loss: 0.0984 || 10iter: 54.1410 sec.\n",
      "イタレーション 760 || Loss: 0.1137 || 10iter: 54.2167 sec.\n",
      "イタレーション 770 || Loss: 0.0321 || 10iter: 54.1944 sec.\n",
      "イタレーション 780 || Loss: 0.0704 || 10iter: 54.2625 sec.\n",
      "イタレーション 790 || Loss: 0.0823 || 10iter: 53.7715 sec.\n",
      "イタレーション 800 || Loss: 0.0663 || 10iter: 54.1368 sec.\n",
      "イタレーション 810 || Loss: 0.0506 || 10iter: 54.1247 sec.\n",
      "イタレーション 820 || Loss: 0.0583 || 10iter: 54.6957 sec.\n",
      "イタレーション 830 || Loss: 0.0915 || 10iter: 54.2621 sec.\n",
      "イタレーション 840 || Loss: 0.0989 || 10iter: 54.1787 sec.\n",
      "イタレーション 850 || Loss: 0.1144 || 10iter: 54.0878 sec.\n",
      "イタレーション 860 || Loss: 0.0353 || 10iter: 54.0776 sec.\n",
      "イタレーション 870 || Loss: 0.0750 || 10iter: 54.1646 sec.\n",
      "イタレーション 880 || Loss: 0.0320 || 10iter: 54.1264 sec.\n",
      "イタレーション 890 || Loss: 0.0355 || 10iter: 54.1188 sec.\n",
      "イタレーション 900 || Loss: 0.0356 || 10iter: 54.7423 sec.\n",
      "イタレーション 910 || Loss: 0.0548 || 10iter: 54.0892 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:0.0661 ||Epoch_VAL_Loss:0.0799\n",
      "timer:  1465.7976 sec.\n",
      "-------------\n",
      "Epoch 6/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 920 || Loss: 0.0555 || 10iter: 24.1445 sec.\n",
      "イタレーション 930 || Loss: 0.0488 || 10iter: 54.0863 sec.\n",
      "イタレーション 940 || Loss: 0.0909 || 10iter: 54.1627 sec.\n",
      "イタレーション 950 || Loss: 0.0632 || 10iter: 54.2467 sec.\n",
      "イタレーション 960 || Loss: 0.0576 || 10iter: 54.1248 sec.\n",
      "イタレーション 970 || Loss: 0.0493 || 10iter: 54.0066 sec.\n",
      "イタレーション 980 || Loss: 0.0806 || 10iter: 54.0686 sec.\n",
      "イタレーション 990 || Loss: 0.0910 || 10iter: 54.0930 sec.\n",
      "イタレーション 1000 || Loss: 0.1456 || 10iter: 54.1929 sec.\n",
      "イタレーション 1010 || Loss: 0.0444 || 10iter: 54.0999 sec.\n",
      "イタレーション 1020 || Loss: 0.0669 || 10iter: 54.0645 sec.\n",
      "イタレーション 1030 || Loss: 0.0586 || 10iter: 54.1186 sec.\n",
      "イタレーション 1040 || Loss: 0.0643 || 10iter: 53.9686 sec.\n",
      "イタレーション 1050 || Loss: 0.0565 || 10iter: 54.2137 sec.\n",
      "イタレーション 1060 || Loss: 0.0697 || 10iter: 54.5902 sec.\n",
      "イタレーション 1070 || Loss: 0.0378 || 10iter: 54.3395 sec.\n",
      "イタレーション 1080 || Loss: 0.0499 || 10iter: 54.1111 sec.\n",
      "イタレーション 1090 || Loss: 0.0611 || 10iter: 54.1203 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:0.0608 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.1238 sec.\n",
      "-------------\n",
      "Epoch 7/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 1100 || Loss: 0.0600 || 10iter: 6.3635 sec.\n",
      "イタレーション 1110 || Loss: 0.1198 || 10iter: 54.0979 sec.\n",
      "イタレーション 1120 || Loss: 0.0719 || 10iter: 54.1431 sec.\n",
      "イタレーション 1130 || Loss: 0.0738 || 10iter: 54.0763 sec.\n",
      "イタレーション 1140 || Loss: 0.0821 || 10iter: 54.0289 sec.\n",
      "イタレーション 1150 || Loss: 0.0370 || 10iter: 54.2141 sec.\n",
      "イタレーション 1160 || Loss: 0.0586 || 10iter: 54.1481 sec.\n",
      "イタレーション 1170 || Loss: 0.0532 || 10iter: 54.1050 sec.\n",
      "イタレーション 1180 || Loss: 0.0511 || 10iter: 54.0222 sec.\n",
      "イタレーション 1190 || Loss: 0.0927 || 10iter: 54.5046 sec.\n",
      "イタレーション 1200 || Loss: 0.0614 || 10iter: 54.5380 sec.\n",
      "イタレーション 1210 || Loss: 0.0674 || 10iter: 54.1348 sec.\n",
      "イタレーション 1220 || Loss: 0.0619 || 10iter: 54.1891 sec.\n",
      "イタレーション 1230 || Loss: 0.0508 || 10iter: 54.0535 sec.\n",
      "イタレーション 1240 || Loss: 0.0582 || 10iter: 54.0170 sec.\n",
      "イタレーション 1250 || Loss: 0.0475 || 10iter: 54.0975 sec.\n",
      "イタレーション 1260 || Loss: 0.0960 || 10iter: 54.6963 sec.\n",
      "イタレーション 1270 || Loss: 0.0363 || 10iter: 54.1836 sec.\n",
      "イタレーション 1280 || Loss: 0.0687 || 10iter: 54.3528 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:0.0597 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.1550 sec.\n",
      "-------------\n",
      "Epoch 8/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 1290 || Loss: 0.0410 || 10iter: 48.1768 sec.\n",
      "イタレーション 1300 || Loss: 0.0526 || 10iter: 54.1858 sec.\n",
      "イタレーション 1310 || Loss: 0.0373 || 10iter: 54.1309 sec.\n",
      "イタレーション 1320 || Loss: 0.0904 || 10iter: 54.0936 sec.\n",
      "イタレーション 1330 || Loss: 0.0570 || 10iter: 54.1174 sec.\n",
      "イタレーション 1340 || Loss: 0.0593 || 10iter: 54.1345 sec.\n",
      "イタレーション 1350 || Loss: 0.0640 || 10iter: 54.2291 sec.\n",
      "イタレーション 1360 || Loss: 0.0691 || 10iter: 54.1497 sec.\n",
      "イタレーション 1370 || Loss: 0.0789 || 10iter: 54.2272 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イタレーション 1380 || Loss: 0.0702 || 10iter: 54.0981 sec.\n",
      "イタレーション 1390 || Loss: 0.0781 || 10iter: 54.1621 sec.\n",
      "イタレーション 1400 || Loss: 0.0591 || 10iter: 54.0377 sec.\n",
      "イタレーション 1410 || Loss: 0.0693 || 10iter: 54.1390 sec.\n",
      "イタレーション 1420 || Loss: 0.0748 || 10iter: 54.5626 sec.\n",
      "イタレーション 1430 || Loss: 0.1073 || 10iter: 54.2815 sec.\n",
      "イタレーション 1440 || Loss: 0.0624 || 10iter: 54.0759 sec.\n",
      "イタレーション 1450 || Loss: 0.0553 || 10iter: 54.1870 sec.\n",
      "イタレーション 1460 || Loss: 0.0365 || 10iter: 54.1660 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:0.0569 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.9084 sec.\n",
      "-------------\n",
      "Epoch 9/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 1470 || Loss: 0.0465 || 10iter: 30.6023 sec.\n",
      "イタレーション 1480 || Loss: 0.0206 || 10iter: 54.9038 sec.\n",
      "イタレーション 1490 || Loss: 0.0392 || 10iter: 54.1856 sec.\n",
      "イタレーション 1500 || Loss: 0.0461 || 10iter: 54.1716 sec.\n",
      "イタレーション 1510 || Loss: 0.0357 || 10iter: 54.2890 sec.\n",
      "イタレーション 1520 || Loss: 0.0358 || 10iter: 54.0945 sec.\n",
      "イタレーション 1530 || Loss: 0.1017 || 10iter: 54.1214 sec.\n",
      "イタレーション 1540 || Loss: 0.0508 || 10iter: 54.1333 sec.\n",
      "イタレーション 1550 || Loss: 0.0854 || 10iter: 54.0759 sec.\n",
      "イタレーション 1560 || Loss: 0.0871 || 10iter: 54.1285 sec.\n",
      "イタレーション 1570 || Loss: 0.0440 || 10iter: 54.1463 sec.\n",
      "イタレーション 1580 || Loss: 0.0599 || 10iter: 54.1660 sec.\n",
      "イタレーション 1590 || Loss: 0.0561 || 10iter: 54.1674 sec.\n",
      "イタレーション 1600 || Loss: 0.0465 || 10iter: 54.0928 sec.\n",
      "イタレーション 1610 || Loss: 0.0451 || 10iter: 54.1039 sec.\n",
      "イタレーション 1620 || Loss: 0.0616 || 10iter: 54.2093 sec.\n",
      "イタレーション 1630 || Loss: 0.0551 || 10iter: 54.1579 sec.\n",
      "イタレーション 1640 || Loss: 0.0411 || 10iter: 54.0807 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:0.0539 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.2394 sec.\n",
      "-------------\n",
      "Epoch 10/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 1650 || Loss: 0.0486 || 10iter: 12.2200 sec.\n",
      "イタレーション 1660 || Loss: 0.0451 || 10iter: 54.1417 sec.\n",
      "イタレーション 1670 || Loss: 0.0616 || 10iter: 54.2311 sec.\n",
      "イタレーション 1680 || Loss: 0.0419 || 10iter: 54.1437 sec.\n",
      "イタレーション 1690 || Loss: 0.0476 || 10iter: 54.1727 sec.\n",
      "イタレーション 1700 || Loss: 0.0680 || 10iter: 54.0519 sec.\n",
      "イタレーション 1710 || Loss: 0.0393 || 10iter: 54.2425 sec.\n",
      "イタレーション 1720 || Loss: 0.0530 || 10iter: 54.5259 sec.\n",
      "イタレーション 1730 || Loss: 0.0461 || 10iter: 54.3420 sec.\n",
      "イタレーション 1740 || Loss: 0.0406 || 10iter: 54.1493 sec.\n",
      "イタレーション 1750 || Loss: 0.0630 || 10iter: 54.0751 sec.\n",
      "イタレーション 1760 || Loss: 0.0579 || 10iter: 54.0079 sec.\n",
      "イタレーション 1770 || Loss: 0.0693 || 10iter: 54.1344 sec.\n",
      "イタレーション 1780 || Loss: 0.0539 || 10iter: 54.3181 sec.\n",
      "イタレーション 1790 || Loss: 0.0373 || 10iter: 54.3498 sec.\n",
      "イタレーション 1800 || Loss: 0.0406 || 10iter: 54.0290 sec.\n",
      "イタレーション 1810 || Loss: 0.0579 || 10iter: 54.2773 sec.\n",
      "イタレーション 1820 || Loss: 0.0626 || 10iter: 54.1745 sec.\n",
      "イタレーション 1830 || Loss: 0.0795 || 10iter: 54.1194 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:0.0501 ||Epoch_VAL_Loss:0.0756\n",
      "timer:  1465.8335 sec.\n",
      "-------------\n",
      "Epoch 11/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 1840 || Loss: 0.0744 || 10iter: 54.2242 sec.\n",
      "イタレーション 1850 || Loss: 0.0543 || 10iter: 54.2046 sec.\n",
      "イタレーション 1860 || Loss: 0.0391 || 10iter: 54.5128 sec.\n",
      "イタレーション 1870 || Loss: 0.0475 || 10iter: 54.1366 sec.\n",
      "イタレーション 1880 || Loss: 0.0454 || 10iter: 54.1117 sec.\n",
      "イタレーション 1890 || Loss: 0.0514 || 10iter: 54.1502 sec.\n",
      "イタレーション 1900 || Loss: 0.0646 || 10iter: 54.2115 sec.\n",
      "イタレーション 1910 || Loss: 0.0497 || 10iter: 54.2050 sec.\n",
      "イタレーション 1920 || Loss: 0.0513 || 10iter: 54.1540 sec.\n",
      "イタレーション 1930 || Loss: 0.0391 || 10iter: 54.0754 sec.\n",
      "イタレーション 1940 || Loss: 0.0394 || 10iter: 54.1238 sec.\n",
      "イタレーション 1950 || Loss: 0.0350 || 10iter: 54.1984 sec.\n",
      "イタレーション 1960 || Loss: 0.0685 || 10iter: 54.1631 sec.\n",
      "イタレーション 1970 || Loss: 0.0371 || 10iter: 54.4586 sec.\n",
      "イタレーション 1980 || Loss: 0.0317 || 10iter: 54.4412 sec.\n",
      "イタレーション 1990 || Loss: 0.0385 || 10iter: 54.0866 sec.\n",
      "イタレーション 2000 || Loss: 0.0610 || 10iter: 54.0567 sec.\n",
      "イタレーション 2010 || Loss: 0.0381 || 10iter: 54.1652 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:0.0491 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.2371 sec.\n",
      "-------------\n",
      "Epoch 12/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2020 || Loss: 0.0454 || 10iter: 36.1468 sec.\n",
      "イタレーション 2030 || Loss: 0.0388 || 10iter: 54.0326 sec.\n",
      "イタレーション 2040 || Loss: 0.0480 || 10iter: 53.9939 sec.\n",
      "イタレーション 2050 || Loss: 0.0543 || 10iter: 54.1971 sec.\n",
      "イタレーション 2060 || Loss: 0.0345 || 10iter: 54.0798 sec.\n",
      "イタレーション 2070 || Loss: 0.0449 || 10iter: 54.2303 sec.\n",
      "イタレーション 2080 || Loss: 0.0348 || 10iter: 54.1950 sec.\n",
      "イタレーション 2090 || Loss: 0.0638 || 10iter: 54.0665 sec.\n",
      "イタレーション 2100 || Loss: 0.0579 || 10iter: 54.1128 sec.\n",
      "イタレーション 2110 || Loss: 0.0686 || 10iter: 54.6431 sec.\n",
      "イタレーション 2120 || Loss: 0.0541 || 10iter: 54.2590 sec.\n",
      "イタレーション 2130 || Loss: 0.0416 || 10iter: 53.9815 sec.\n",
      "イタレーション 2140 || Loss: 0.0334 || 10iter: 54.0707 sec.\n",
      "イタレーション 2150 || Loss: 0.0250 || 10iter: 54.1068 sec.\n",
      "イタレーション 2160 || Loss: 0.0379 || 10iter: 54.1345 sec.\n",
      "イタレーション 2170 || Loss: 0.1628 || 10iter: 54.1746 sec.\n",
      "イタレーション 2180 || Loss: 0.0530 || 10iter: 54.7445 sec.\n",
      "イタレーション 2190 || Loss: 0.0367 || 10iter: 54.0350 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:0.0479 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.6695 sec.\n",
      "-------------\n",
      "Epoch 13/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2200 || Loss: 0.0587 || 10iter: 18.2050 sec.\n",
      "イタレーション 2210 || Loss: 0.0502 || 10iter: 54.1402 sec.\n",
      "イタレーション 2220 || Loss: 0.0587 || 10iter: 54.1186 sec.\n",
      "イタレーション 2230 || Loss: 0.0506 || 10iter: 54.1087 sec.\n",
      "イタレーション 2240 || Loss: 0.0380 || 10iter: 54.2705 sec.\n",
      "イタレーション 2250 || Loss: 0.0411 || 10iter: 54.4019 sec.\n",
      "イタレーション 2260 || Loss: 0.0399 || 10iter: 54.1756 sec.\n",
      "イタレーション 2270 || Loss: 0.0344 || 10iter: 54.2592 sec.\n",
      "イタレーション 2280 || Loss: 0.0458 || 10iter: 54.1004 sec.\n",
      "イタレーション 2290 || Loss: 0.0316 || 10iter: 53.9885 sec.\n",
      "イタレーション 2300 || Loss: 0.0431 || 10iter: 54.1248 sec.\n",
      "イタレーション 2310 || Loss: 0.0340 || 10iter: 54.1556 sec.\n",
      "イタレーション 2320 || Loss: 0.0469 || 10iter: 54.2401 sec.\n",
      "イタレーション 2330 || Loss: 0.0263 || 10iter: 54.2122 sec.\n",
      "イタレーション 2340 || Loss: 0.0508 || 10iter: 54.0861 sec.\n",
      "イタレーション 2350 || Loss: 0.0257 || 10iter: 54.2221 sec.\n",
      "イタレーション 2360 || Loss: 0.0440 || 10iter: 54.1030 sec.\n",
      "イタレーション 2370 || Loss: 0.0919 || 10iter: 54.1321 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:0.0463 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.4704 sec.\n",
      "-------------\n",
      "Epoch 14/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2380 || Loss: 0.0214 || 10iter: 0.3352 sec.\n",
      "イタレーション 2390 || Loss: 0.0363 || 10iter: 54.0246 sec.\n",
      "イタレーション 2400 || Loss: 0.0381 || 10iter: 54.1731 sec.\n",
      "イタレーション 2410 || Loss: 0.0331 || 10iter: 54.6710 sec.\n",
      "イタレーション 2420 || Loss: 0.0476 || 10iter: 54.2393 sec.\n",
      "イタレーション 2430 || Loss: 0.0550 || 10iter: 54.0183 sec.\n",
      "イタレーション 2440 || Loss: 0.0579 || 10iter: 54.0719 sec.\n",
      "イタレーション 2450 || Loss: 0.0597 || 10iter: 54.1087 sec.\n",
      "イタレーション 2460 || Loss: 0.0415 || 10iter: 54.1573 sec.\n",
      "イタレーション 2470 || Loss: 0.0421 || 10iter: 54.6312 sec.\n",
      "イタレーション 2480 || Loss: 0.0305 || 10iter: 54.1641 sec.\n",
      "イタレーション 2490 || Loss: 0.0452 || 10iter: 54.0090 sec.\n",
      "イタレーション 2500 || Loss: 0.0536 || 10iter: 54.1060 sec.\n",
      "イタレーション 2510 || Loss: 0.0433 || 10iter: 54.0351 sec.\n",
      "イタレーション 2520 || Loss: 0.0432 || 10iter: 54.1337 sec.\n",
      "イタレーション 2530 || Loss: 0.0747 || 10iter: 54.2507 sec.\n",
      "イタレーション 2540 || Loss: 0.0455 || 10iter: 54.2375 sec.\n",
      "イタレーション 2550 || Loss: 0.0492 || 10iter: 54.1312 sec.\n",
      "イタレーション 2560 || Loss: 0.0283 || 10iter: 54.1643 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:0.0458 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.0335 sec.\n",
      "-------------\n",
      "Epoch 15/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2570 || Loss: 0.0351 || 10iter: 42.1955 sec.\n",
      "イタレーション 2580 || Loss: 0.0416 || 10iter: 54.0698 sec.\n",
      "イタレーション 2590 || Loss: 0.0312 || 10iter: 54.2115 sec.\n",
      "イタレーション 2600 || Loss: 0.0346 || 10iter: 54.2867 sec.\n",
      "イタレーション 2610 || Loss: 0.0452 || 10iter: 54.1475 sec.\n",
      "イタレーション 2620 || Loss: 0.0267 || 10iter: 54.1695 sec.\n",
      "イタレーション 2630 || Loss: 0.0339 || 10iter: 54.1613 sec.\n",
      "イタレーション 2640 || Loss: 0.0603 || 10iter: 54.0406 sec.\n",
      "イタレーション 2650 || Loss: 0.0485 || 10iter: 54.0792 sec.\n",
      "イタレーション 2660 || Loss: 0.0517 || 10iter: 54.0815 sec.\n",
      "イタレーション 2670 || Loss: 0.0366 || 10iter: 54.1041 sec.\n",
      "イタレーション 2680 || Loss: 0.0397 || 10iter: 54.1817 sec.\n",
      "イタレーション 2690 || Loss: 0.0266 || 10iter: 54.2652 sec.\n",
      "イタレーション 2700 || Loss: 0.0381 || 10iter: 54.1737 sec.\n",
      "イタレーション 2710 || Loss: 0.0564 || 10iter: 54.0500 sec.\n",
      "イタレーション 2720 || Loss: 0.0334 || 10iter: 54.1115 sec.\n",
      "イタレーション 2730 || Loss: 0.0472 || 10iter: 54.0948 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イタレーション 2740 || Loss: 0.0266 || 10iter: 54.1817 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:0.0454 ||Epoch_VAL_Loss:0.0712\n",
      "timer:  1465.1200 sec.\n",
      "-------------\n",
      "Epoch 16/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2750 || Loss: 0.0719 || 10iter: 24.3353 sec.\n",
      "イタレーション 2760 || Loss: 0.0291 || 10iter: 54.0146 sec.\n",
      "イタレーション 2770 || Loss: 0.0289 || 10iter: 53.9887 sec.\n",
      "イタレーション 2780 || Loss: 0.0385 || 10iter: 54.1129 sec.\n",
      "イタレーション 2790 || Loss: 0.0379 || 10iter: 54.0923 sec.\n",
      "イタレーション 2800 || Loss: 0.0459 || 10iter: 54.0815 sec.\n",
      "イタレーション 2810 || Loss: 0.0315 || 10iter: 54.1502 sec.\n",
      "イタレーション 2820 || Loss: 0.0307 || 10iter: 54.1893 sec.\n",
      "イタレーション 2830 || Loss: 0.0475 || 10iter: 53.9634 sec.\n",
      "イタレーション 2840 || Loss: 0.0258 || 10iter: 54.0413 sec.\n",
      "イタレーション 2850 || Loss: 0.0435 || 10iter: 54.2436 sec.\n",
      "イタレーション 2860 || Loss: 0.0519 || 10iter: 54.1655 sec.\n",
      "イタレーション 2870 || Loss: 0.0288 || 10iter: 54.1621 sec.\n",
      "イタレーション 2880 || Loss: 0.0290 || 10iter: 53.9877 sec.\n",
      "イタレーション 2890 || Loss: 0.0364 || 10iter: 54.0130 sec.\n",
      "イタレーション 2900 || Loss: 0.0351 || 10iter: 54.1669 sec.\n",
      "イタレーション 2910 || Loss: 0.0396 || 10iter: 53.7070 sec.\n",
      "イタレーション 2920 || Loss: 0.0460 || 10iter: 54.3912 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:0.0424 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1093.8560 sec.\n",
      "-------------\n",
      "Epoch 17/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 2930 || Loss: 0.0489 || 10iter: 6.2268 sec.\n",
      "イタレーション 2940 || Loss: 0.0502 || 10iter: 54.2730 sec.\n",
      "イタレーション 2950 || Loss: 0.0482 || 10iter: 54.3103 sec.\n",
      "イタレーション 2960 || Loss: 0.0540 || 10iter: 53.8701 sec.\n",
      "イタレーション 2970 || Loss: 0.0404 || 10iter: 54.3012 sec.\n",
      "イタレーション 2980 || Loss: 0.0510 || 10iter: 54.3097 sec.\n",
      "イタレーション 2990 || Loss: 0.0515 || 10iter: 54.0885 sec.\n",
      "イタレーション 3000 || Loss: 0.0397 || 10iter: 54.1652 sec.\n",
      "イタレーション 3010 || Loss: 0.0351 || 10iter: 54.3906 sec.\n",
      "イタレーション 3020 || Loss: 0.0259 || 10iter: 54.2828 sec.\n",
      "イタレーション 3030 || Loss: 0.0460 || 10iter: 54.0992 sec.\n",
      "イタレーション 3040 || Loss: 0.0383 || 10iter: 54.0833 sec.\n",
      "イタレーション 3050 || Loss: 0.0584 || 10iter: 54.1737 sec.\n",
      "イタレーション 3060 || Loss: 0.0268 || 10iter: 54.1729 sec.\n",
      "イタレーション 3070 || Loss: 0.0280 || 10iter: 54.2713 sec.\n",
      "イタレーション 3080 || Loss: 0.0700 || 10iter: 54.2481 sec.\n",
      "イタレーション 3090 || Loss: 0.0359 || 10iter: 54.3508 sec.\n",
      "イタレーション 3100 || Loss: 0.0469 || 10iter: 54.1454 sec.\n",
      "イタレーション 3110 || Loss: 0.0497 || 10iter: 54.4992 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:0.0443 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.5816 sec.\n",
      "-------------\n",
      "Epoch 18/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 3120 || Loss: 0.0297 || 10iter: 48.3740 sec.\n",
      "イタレーション 3130 || Loss: 0.0469 || 10iter: 54.1072 sec.\n",
      "イタレーション 3140 || Loss: 0.0338 || 10iter: 54.0433 sec.\n",
      "イタレーション 3150 || Loss: 0.0496 || 10iter: 54.2346 sec.\n",
      "イタレーション 3160 || Loss: 0.0281 || 10iter: 54.2297 sec.\n",
      "イタレーション 3170 || Loss: 0.0265 || 10iter: 54.2628 sec.\n",
      "イタレーション 3180 || Loss: 0.0314 || 10iter: 54.0549 sec.\n",
      "イタレーション 3190 || Loss: 0.0415 || 10iter: 54.4177 sec.\n",
      "イタレーション 3200 || Loss: 0.0952 || 10iter: 54.0846 sec.\n",
      "イタレーション 3210 || Loss: 0.0422 || 10iter: 54.2877 sec.\n",
      "イタレーション 3220 || Loss: 0.0341 || 10iter: 54.1686 sec.\n",
      "イタレーション 3230 || Loss: 0.0312 || 10iter: 54.0703 sec.\n",
      "イタレーション 3240 || Loss: 0.0422 || 10iter: 54.0119 sec.\n",
      "イタレーション 3250 || Loss: 0.0484 || 10iter: 54.0988 sec.\n",
      "イタレーション 3260 || Loss: 0.0418 || 10iter: 54.0245 sec.\n",
      "イタレーション 3270 || Loss: 0.0507 || 10iter: 54.1907 sec.\n",
      "イタレーション 3280 || Loss: 0.0324 || 10iter: 54.1526 sec.\n",
      "イタレーション 3290 || Loss: 0.0355 || 10iter: 54.0668 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:0.0432 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.4396 sec.\n",
      "-------------\n",
      "Epoch 19/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 3300 || Loss: 0.0446 || 10iter: 30.1254 sec.\n",
      "イタレーション 3310 || Loss: 0.0850 || 10iter: 54.3403 sec.\n",
      "イタレーション 3320 || Loss: 0.0476 || 10iter: 54.3215 sec.\n",
      "イタレーション 3330 || Loss: 0.0292 || 10iter: 54.1830 sec.\n",
      "イタレーション 3340 || Loss: 0.0640 || 10iter: 54.3660 sec.\n",
      "イタレーション 3350 || Loss: 0.0436 || 10iter: 54.0220 sec.\n",
      "イタレーション 3360 || Loss: 0.0385 || 10iter: 54.1266 sec.\n",
      "イタレーション 3370 || Loss: 0.0297 || 10iter: 54.2117 sec.\n",
      "イタレーション 3380 || Loss: 0.0332 || 10iter: 54.2333 sec.\n",
      "イタレーション 3390 || Loss: 0.0303 || 10iter: 54.1200 sec.\n",
      "イタレーション 3400 || Loss: 0.0362 || 10iter: 54.1320 sec.\n",
      "イタレーション 3410 || Loss: 0.0362 || 10iter: 54.0548 sec.\n",
      "イタレーション 3420 || Loss: 0.0468 || 10iter: 54.0577 sec.\n",
      "イタレーション 3430 || Loss: 0.0383 || 10iter: 54.1121 sec.\n",
      "イタレーション 3440 || Loss: 0.0493 || 10iter: 54.5792 sec.\n",
      "イタレーション 3450 || Loss: 0.0312 || 10iter: 54.1596 sec.\n",
      "イタレーション 3460 || Loss: 0.0293 || 10iter: 54.0807 sec.\n",
      "イタレーション 3470 || Loss: 0.0393 || 10iter: 54.0923 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:0.0429 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.8596 sec.\n",
      "-------------\n",
      "Epoch 20/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 3480 || Loss: 0.0246 || 10iter: 12.3417 sec.\n",
      "イタレーション 3490 || Loss: 0.0384 || 10iter: 53.9844 sec.\n",
      "イタレーション 3500 || Loss: 0.0584 || 10iter: 54.0588 sec.\n",
      "イタレーション 3510 || Loss: 0.0301 || 10iter: 54.1150 sec.\n",
      "イタレーション 3520 || Loss: 0.0201 || 10iter: 54.1780 sec.\n",
      "イタレーション 3530 || Loss: 0.0420 || 10iter: 54.1976 sec.\n",
      "イタレーション 3540 || Loss: 0.0380 || 10iter: 54.2860 sec.\n",
      "イタレーション 3550 || Loss: 0.0462 || 10iter: 54.2149 sec.\n",
      "イタレーション 3560 || Loss: 0.0673 || 10iter: 54.1770 sec.\n",
      "イタレーション 3570 || Loss: 0.0341 || 10iter: 54.1633 sec.\n",
      "イタレーション 3580 || Loss: 0.0338 || 10iter: 54.2060 sec.\n",
      "イタレーション 3590 || Loss: 0.0400 || 10iter: 54.1625 sec.\n",
      "イタレーション 3600 || Loss: 0.0790 || 10iter: 54.1928 sec.\n",
      "イタレーション 3610 || Loss: 0.0515 || 10iter: 54.0409 sec.\n",
      "イタレーション 3620 || Loss: 0.0509 || 10iter: 54.0782 sec.\n",
      "イタレーション 3630 || Loss: 0.0221 || 10iter: 54.0888 sec.\n",
      "イタレーション 3640 || Loss: 0.0220 || 10iter: 54.4018 sec.\n",
      "イタレーション 3650 || Loss: 0.0506 || 10iter: 54.2259 sec.\n",
      "イタレーション 3660 || Loss: 0.0394 || 10iter: 54.8280 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:0.0426 ||Epoch_VAL_Loss:0.0713\n",
      "timer:  1466.0158 sec.\n",
      "-------------\n",
      "Epoch 21/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 3670 || Loss: 0.0718 || 10iter: 54.2615 sec.\n",
      "イタレーション 3680 || Loss: 0.0619 || 10iter: 54.0667 sec.\n",
      "イタレーション 3690 || Loss: 0.0326 || 10iter: 54.3560 sec.\n",
      "イタレーション 3700 || Loss: 0.0216 || 10iter: 54.2593 sec.\n",
      "イタレーション 3710 || Loss: 0.0389 || 10iter: 54.3093 sec.\n",
      "イタレーション 3720 || Loss: 0.0422 || 10iter: 54.1579 sec.\n",
      "イタレーション 3730 || Loss: 0.0486 || 10iter: 54.2401 sec.\n",
      "イタレーション 3740 || Loss: 0.0333 || 10iter: 54.2155 sec.\n",
      "イタレーション 3750 || Loss: 0.0479 || 10iter: 54.2482 sec.\n",
      "イタレーション 3760 || Loss: 0.0247 || 10iter: 54.3346 sec.\n",
      "イタレーション 3770 || Loss: 0.0520 || 10iter: 54.2643 sec.\n",
      "イタレーション 3780 || Loss: 0.0674 || 10iter: 54.2208 sec.\n",
      "イタレーション 3790 || Loss: 0.0344 || 10iter: 54.1820 sec.\n",
      "イタレーション 3800 || Loss: 0.0525 || 10iter: 54.0820 sec.\n",
      "イタレーション 3810 || Loss: 0.0382 || 10iter: 54.2764 sec.\n",
      "イタレーション 3820 || Loss: 0.0478 || 10iter: 54.2514 sec.\n",
      "イタレーション 3830 || Loss: 0.0413 || 10iter: 54.2036 sec.\n",
      "イタレーション 3840 || Loss: 0.0228 || 10iter: 54.2089 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:0.0417 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.8858 sec.\n",
      "-------------\n",
      "Epoch 22/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 3850 || Loss: 0.0237 || 10iter: 36.3271 sec.\n",
      "イタレーション 3860 || Loss: 0.0589 || 10iter: 54.3126 sec.\n",
      "イタレーション 3870 || Loss: 0.0231 || 10iter: 54.2298 sec.\n",
      "イタレーション 3880 || Loss: 0.0296 || 10iter: 54.2262 sec.\n",
      "イタレーション 3890 || Loss: 0.0454 || 10iter: 54.1649 sec.\n",
      "イタレーション 3900 || Loss: 0.0383 || 10iter: 54.2550 sec.\n",
      "イタレーション 3910 || Loss: 0.0415 || 10iter: 54.2455 sec.\n",
      "イタレーション 3920 || Loss: 0.0377 || 10iter: 54.2546 sec.\n",
      "イタレーション 3930 || Loss: 0.0691 || 10iter: 54.2230 sec.\n",
      "イタレーション 3940 || Loss: 0.0285 || 10iter: 54.2498 sec.\n",
      "イタレーション 3950 || Loss: 0.0634 || 10iter: 54.2872 sec.\n",
      "イタレーション 3960 || Loss: 0.0373 || 10iter: 54.2767 sec.\n",
      "イタレーション 3970 || Loss: 0.0297 || 10iter: 54.4924 sec.\n",
      "イタレーション 3980 || Loss: 0.0201 || 10iter: 54.1666 sec.\n",
      "イタレーション 3990 || Loss: 0.0305 || 10iter: 54.3984 sec.\n",
      "イタレーション 4000 || Loss: 0.0381 || 10iter: 54.1296 sec.\n",
      "イタレーション 4010 || Loss: 0.0275 || 10iter: 54.2219 sec.\n",
      "イタレーション 4020 || Loss: 0.0296 || 10iter: 54.1934 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:0.0425 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1096.3768 sec.\n",
      "-------------\n",
      "Epoch 23/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4030 || Loss: 0.0520 || 10iter: 18.3429 sec.\n",
      "イタレーション 4040 || Loss: 0.0358 || 10iter: 54.1506 sec.\n",
      "イタレーション 4050 || Loss: 0.0648 || 10iter: 54.1812 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イタレーション 4060 || Loss: 0.0293 || 10iter: 54.2066 sec.\n",
      "イタレーション 4070 || Loss: 0.0239 || 10iter: 54.2813 sec.\n",
      "イタレーション 4080 || Loss: 0.0662 || 10iter: 54.3449 sec.\n",
      "イタレーション 4090 || Loss: 0.0234 || 10iter: 54.3371 sec.\n",
      "イタレーション 4100 || Loss: 0.0332 || 10iter: 54.2052 sec.\n",
      "イタレーション 4110 || Loss: 0.0284 || 10iter: 54.2600 sec.\n",
      "イタレーション 4120 || Loss: 0.0401 || 10iter: 54.4572 sec.\n",
      "イタレーション 4130 || Loss: 0.0499 || 10iter: 54.3100 sec.\n",
      "イタレーション 4140 || Loss: 0.0366 || 10iter: 54.3269 sec.\n",
      "イタレーション 4150 || Loss: 0.0455 || 10iter: 54.5363 sec.\n",
      "イタレーション 4160 || Loss: 0.0442 || 10iter: 54.2133 sec.\n",
      "イタレーション 4170 || Loss: 0.0750 || 10iter: 54.3995 sec.\n",
      "イタレーション 4180 || Loss: 0.0446 || 10iter: 54.3855 sec.\n",
      "イタレーション 4190 || Loss: 0.0391 || 10iter: 54.1874 sec.\n",
      "イタレーション 4200 || Loss: 0.0412 || 10iter: 54.2442 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:0.0417 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1097.0536 sec.\n",
      "-------------\n",
      "Epoch 24/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4210 || Loss: 0.0190 || 10iter: 0.3346 sec.\n",
      "イタレーション 4220 || Loss: 0.0374 || 10iter: 54.3153 sec.\n",
      "イタレーション 4230 || Loss: 0.0535 || 10iter: 54.1959 sec.\n",
      "イタレーション 4240 || Loss: 0.0614 || 10iter: 54.3174 sec.\n",
      "イタレーション 4250 || Loss: 0.0228 || 10iter: 54.0893 sec.\n",
      "イタレーション 4260 || Loss: 0.0296 || 10iter: 54.2555 sec.\n",
      "イタレーション 4270 || Loss: 0.0397 || 10iter: 54.3673 sec.\n",
      "イタレーション 4280 || Loss: 0.0938 || 10iter: 54.2864 sec.\n",
      "イタレーション 4290 || Loss: 0.0623 || 10iter: 54.1896 sec.\n",
      "イタレーション 4300 || Loss: 0.0386 || 10iter: 54.6921 sec.\n",
      "イタレーション 4310 || Loss: 0.0264 || 10iter: 54.2042 sec.\n",
      "イタレーション 4320 || Loss: 0.0168 || 10iter: 54.1686 sec.\n",
      "イタレーション 4330 || Loss: 0.0420 || 10iter: 54.3366 sec.\n",
      "イタレーション 4340 || Loss: 0.0524 || 10iter: 54.2998 sec.\n",
      "イタレーション 4350 || Loss: 0.0340 || 10iter: 54.2215 sec.\n",
      "イタレーション 4360 || Loss: 0.0255 || 10iter: 54.1752 sec.\n",
      "イタレーション 4370 || Loss: 0.0428 || 10iter: 54.3470 sec.\n",
      "イタレーション 4380 || Loss: 0.0570 || 10iter: 54.2072 sec.\n",
      "イタレーション 4390 || Loss: 0.0244 || 10iter: 54.3469 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:0.0408 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1096.5887 sec.\n",
      "-------------\n",
      "Epoch 25/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4400 || Loss: 0.0413 || 10iter: 42.1758 sec.\n",
      "イタレーション 4410 || Loss: 0.0357 || 10iter: 54.1372 sec.\n",
      "イタレーション 4420 || Loss: 0.0320 || 10iter: 54.9679 sec.\n",
      "イタレーション 4430 || Loss: 0.0388 || 10iter: 54.4850 sec.\n",
      "イタレーション 4440 || Loss: 0.0448 || 10iter: 54.1916 sec.\n",
      "イタレーション 4450 || Loss: 0.0314 || 10iter: 54.8733 sec.\n",
      "イタレーション 4460 || Loss: 0.0578 || 10iter: 54.3017 sec.\n",
      "イタレーション 4470 || Loss: 0.0225 || 10iter: 54.3136 sec.\n",
      "イタレーション 4480 || Loss: 0.0246 || 10iter: 54.4513 sec.\n",
      "イタレーション 4490 || Loss: 0.0217 || 10iter: 54.2976 sec.\n",
      "イタレーション 4500 || Loss: 0.0470 || 10iter: 54.2567 sec.\n",
      "イタレーション 4510 || Loss: 0.0441 || 10iter: 54.2432 sec.\n",
      "イタレーション 4520 || Loss: 0.0460 || 10iter: 54.1697 sec.\n",
      "イタレーション 4530 || Loss: 0.0312 || 10iter: 54.2957 sec.\n",
      "イタレーション 4540 || Loss: 0.0315 || 10iter: 54.3635 sec.\n",
      "イタレーション 4550 || Loss: 0.0394 || 10iter: 54.2669 sec.\n",
      "イタレーション 4560 || Loss: 0.0325 || 10iter: 54.1388 sec.\n",
      "イタレーション 4570 || Loss: 0.0422 || 10iter: 54.3174 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:0.0411 ||Epoch_VAL_Loss:0.0714\n",
      "timer:  1469.5483 sec.\n",
      "-------------\n",
      "Epoch 26/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4580 || Loss: 0.0374 || 10iter: 24.1892 sec.\n",
      "イタレーション 4590 || Loss: 0.0377 || 10iter: 54.2329 sec.\n",
      "イタレーション 4600 || Loss: 0.0496 || 10iter: 54.3100 sec.\n",
      "イタレーション 4610 || Loss: 0.0325 || 10iter: 54.2457 sec.\n",
      "イタレーション 4620 || Loss: 0.0271 || 10iter: 54.1240 sec.\n",
      "イタレーション 4630 || Loss: 0.0298 || 10iter: 54.2757 sec.\n",
      "イタレーション 4640 || Loss: 0.0421 || 10iter: 54.1111 sec.\n",
      "イタレーション 4650 || Loss: 0.0279 || 10iter: 54.2963 sec.\n",
      "イタレーション 4660 || Loss: 0.0472 || 10iter: 54.2724 sec.\n",
      "イタレーション 4670 || Loss: 0.0418 || 10iter: 54.3451 sec.\n",
      "イタレーション 4680 || Loss: 0.0552 || 10iter: 54.2771 sec.\n",
      "イタレーション 4690 || Loss: 0.0233 || 10iter: 54.2222 sec.\n",
      "イタレーション 4700 || Loss: 0.0282 || 10iter: 54.2090 sec.\n",
      "イタレーション 4710 || Loss: 0.0350 || 10iter: 54.2483 sec.\n",
      "イタレーション 4720 || Loss: 0.0477 || 10iter: 54.2537 sec.\n",
      "イタレーション 4730 || Loss: 0.0537 || 10iter: 54.2938 sec.\n",
      "イタレーション 4740 || Loss: 0.0370 || 10iter: 54.0490 sec.\n",
      "イタレーション 4750 || Loss: 0.0285 || 10iter: 54.2259 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:0.0400 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1095.6910 sec.\n",
      "-------------\n",
      "Epoch 27/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4760 || Loss: 0.0482 || 10iter: 6.4042 sec.\n",
      "イタレーション 4770 || Loss: 0.0384 || 10iter: 54.0471 sec.\n",
      "イタレーション 4780 || Loss: 0.0248 || 10iter: 54.2403 sec.\n",
      "イタレーション 4790 || Loss: 0.0337 || 10iter: 54.1062 sec.\n",
      "イタレーション 4800 || Loss: 0.0521 || 10iter: 54.2482 sec.\n",
      "イタレーション 4810 || Loss: 0.0309 || 10iter: 54.1904 sec.\n",
      "イタレーション 4820 || Loss: 0.0291 || 10iter: 54.2232 sec.\n",
      "イタレーション 4830 || Loss: 0.0182 || 10iter: 54.0680 sec.\n",
      "イタレーション 4840 || Loss: 0.0475 || 10iter: 54.0377 sec.\n",
      "イタレーション 4850 || Loss: 0.0590 || 10iter: 54.1410 sec.\n",
      "イタレーション 4860 || Loss: 0.0301 || 10iter: 54.1762 sec.\n",
      "イタレーション 4870 || Loss: 0.0336 || 10iter: 54.1924 sec.\n",
      "イタレーション 4880 || Loss: 0.0407 || 10iter: 54.0745 sec.\n",
      "イタレーション 4890 || Loss: 0.0292 || 10iter: 54.0224 sec.\n",
      "イタレーション 4900 || Loss: 0.0202 || 10iter: 54.0127 sec.\n",
      "イタレーション 4910 || Loss: 0.0385 || 10iter: 53.9551 sec.\n",
      "イタレーション 4920 || Loss: 0.0174 || 10iter: 54.2000 sec.\n",
      "イタレーション 4930 || Loss: 0.0353 || 10iter: 54.0978 sec.\n",
      "イタレーション 4940 || Loss: 0.0298 || 10iter: 54.1059 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:0.0386 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1093.7054 sec.\n",
      "-------------\n",
      "Epoch 28/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 4950 || Loss: 0.0711 || 10iter: 48.0674 sec.\n",
      "イタレーション 4960 || Loss: 0.0332 || 10iter: 54.0982 sec.\n",
      "イタレーション 4970 || Loss: 0.0338 || 10iter: 54.0974 sec.\n",
      "イタレーション 4980 || Loss: 0.0469 || 10iter: 54.0934 sec.\n",
      "イタレーション 4990 || Loss: 0.0461 || 10iter: 54.0375 sec.\n",
      "イタレーション 5000 || Loss: 0.0424 || 10iter: 53.9610 sec.\n",
      "イタレーション 5010 || Loss: 0.0354 || 10iter: 54.0289 sec.\n",
      "イタレーション 5020 || Loss: 0.0429 || 10iter: 54.1131 sec.\n",
      "イタレーション 5030 || Loss: 0.0375 || 10iter: 54.0892 sec.\n",
      "イタレーション 5040 || Loss: 0.0246 || 10iter: 54.0469 sec.\n",
      "イタレーション 5050 || Loss: 0.0251 || 10iter: 54.1910 sec.\n",
      "イタレーション 5060 || Loss: 0.0401 || 10iter: 54.0707 sec.\n",
      "イタレーション 5070 || Loss: 0.0512 || 10iter: 53.9953 sec.\n",
      "イタレーション 5080 || Loss: 0.0399 || 10iter: 54.1119 sec.\n",
      "イタレーション 5090 || Loss: 0.0250 || 10iter: 54.0402 sec.\n",
      "イタレーション 5100 || Loss: 0.0392 || 10iter: 53.9607 sec.\n",
      "イタレーション 5110 || Loss: 0.0194 || 10iter: 53.9674 sec.\n",
      "イタレーション 5120 || Loss: 0.0521 || 10iter: 53.9787 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:0.0403 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1092.5115 sec.\n",
      "-------------\n",
      "Epoch 29/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 5130 || Loss: 0.0302 || 10iter: 30.2511 sec.\n",
      "イタレーション 5140 || Loss: 0.0413 || 10iter: 54.3475 sec.\n",
      "イタレーション 5150 || Loss: 0.0364 || 10iter: 54.0594 sec.\n",
      "イタレーション 5160 || Loss: 0.0590 || 10iter: 54.0514 sec.\n",
      "イタレーション 5170 || Loss: 0.0494 || 10iter: 54.2798 sec.\n",
      "イタレーション 5180 || Loss: 0.0436 || 10iter: 54.0612 sec.\n",
      "イタレーション 5190 || Loss: 0.0395 || 10iter: 54.0327 sec.\n",
      "イタレーション 5200 || Loss: 0.0338 || 10iter: 54.0320 sec.\n",
      "イタレーション 5210 || Loss: 0.0250 || 10iter: 54.1735 sec.\n",
      "イタレーション 5220 || Loss: 0.0330 || 10iter: 54.3132 sec.\n",
      "イタレーション 5230 || Loss: 0.0365 || 10iter: 54.1778 sec.\n",
      "イタレーション 5240 || Loss: 0.0401 || 10iter: 54.1138 sec.\n",
      "イタレーション 5250 || Loss: 0.0573 || 10iter: 54.1316 sec.\n",
      "イタレーション 5260 || Loss: 0.0319 || 10iter: 54.3398 sec.\n",
      "イタレーション 5270 || Loss: 0.0409 || 10iter: 54.0213 sec.\n",
      "イタレーション 5280 || Loss: 0.0610 || 10iter: 54.1354 sec.\n",
      "イタレーション 5290 || Loss: 0.0277 || 10iter: 54.1520 sec.\n",
      "イタレーション 5300 || Loss: 0.0502 || 10iter: 54.1583 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:0.0396 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  1094.3041 sec.\n",
      "-------------\n",
      "Epoch 30/30\n",
      "-------------\n",
      "（train）\n",
      "イタレーション 5310 || Loss: 0.0368 || 10iter: 12.1966 sec.\n",
      "イタレーション 5320 || Loss: 0.0405 || 10iter: 54.1733 sec.\n",
      "イタレーション 5330 || Loss: 0.0234 || 10iter: 54.1175 sec.\n",
      "イタレーション 5340 || Loss: 0.0395 || 10iter: 54.1001 sec.\n",
      "イタレーション 5350 || Loss: 0.0195 || 10iter: 54.3903 sec.\n",
      "イタレーション 5360 || Loss: 0.0247 || 10iter: 53.9680 sec.\n",
      "イタレーション 5370 || Loss: 0.0259 || 10iter: 54.1087 sec.\n",
      "イタレーション 5380 || Loss: 0.0513 || 10iter: 54.0284 sec.\n",
      "イタレーション 5390 || Loss: 0.0507 || 10iter: 54.2891 sec.\n",
      "イタレーション 5400 || Loss: 0.0423 || 10iter: 53.6634 sec.\n",
      "イタレーション 5410 || Loss: 0.0402 || 10iter: 54.2152 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イタレーション 5420 || Loss: 0.0186 || 10iter: 53.7726 sec.\n",
      "イタレーション 5430 || Loss: 0.0413 || 10iter: 53.9767 sec.\n",
      "イタレーション 5440 || Loss: 0.0484 || 10iter: 54.0522 sec.\n",
      "イタレーション 5450 || Loss: 0.0491 || 10iter: 54.0189 sec.\n",
      "イタレーション 5460 || Loss: 0.0440 || 10iter: 54.2314 sec.\n",
      "イタレーション 5470 || Loss: 0.0187 || 10iter: 54.1328 sec.\n",
      "イタレーション 5480 || Loss: 0.0654 || 10iter: 54.1384 sec.\n",
      "イタレーション 5490 || Loss: 0.0625 || 10iter: 54.1472 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:0.0391 ||Epoch_VAL_Loss:0.0698\n",
      "timer:  1463.1491 sec.\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する\n",
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
